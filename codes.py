import warningswarnings.filterwarnings('ignore')import numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)import seaborn as snsimport sklearnimport matplotlib.pyplot as pltimport numpy as npimport pandas as pd import statsmodels.api as smimport statsmodels.formula.api as smfimport seaborn as snsfrom sklearn.preprocessing import scale from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_scorefrom sklearn.metrics import confusion_matrix, accuracy_score, classification_reportfrom sklearn.metrics import roc_auc_score,roc_curvefrom sklearn.metrics import precision_scorefrom sklearn.metrics import recall_scorefrom sklearn.metrics import f1_scorefrom sklearn.metrics import roc_auc_scorefrom sklearn.metrics import aucimport statsmodels.formula.api as smfimport matplotlib.pyplot as pltfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysisfrom sklearn.linear_model import LogisticRegressionfrom sklearn.svm import SVCfrom sklearn.naive_bayes import GaussianNBfrom sklearn import treefrom sklearn.ensemble import RandomForestClassifierfrom sklearn.ensemble import GradientBoostingClassifierfrom xgboost import XGBClassifierfrom sklearn.model_selection import train_test_splitstroke_data = pd.read_csv("healthcare-dataset-stroke-data.csv")df = stroke_data.copy()print("Info: \n", df.info(), "\n---------------")print("Describe: \n", df.describe().T, "\n---------------")print("Null: \n", df.isnull().sum(), "\n---------------")df.drop(['id'], axis = 1, inplace=True)df.bmi.replace(to_replace=np.nan, value=df.bmi.mean(), inplace=True)X = df.drop(['stroke'], axis = 1)y = df['stroke']numerical = X.select_dtypes(include = ['int64', 'float64']).columns.to_list()categorical = X.select_dtypes(include = ['object']).columns.to_list()#Label Encoderfrom sklearn.preprocessing import StandardScalerfrom sklearn.preprocessing import LabelEncoderdef label_encoder(df):    for i in categorical:        le = LabelEncoder()        df[i] = le.fit_transform(df[i])    return df#Standard Scaler    sc = StandardScaler()X[numerical] = sc.fit_transform(X[numerical])# Label encodingX = label_encoder(X)#Smotefrom imblearn.over_sampling import SMOTEX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 22)smote = SMOTE()X_train_balanced, Y_train_balanced = smote.fit_resample(X_train, y_train)#Modelling#Log Regressionlg = LogisticRegression(random_state = 22)lg.fit(X_train_balanced, Y_train_balanced)y_pred = lg.predict(X_test)y_prob = lg.predict_proba(X_test)[:,1]results = pd.DataFrame(columns = ['SVC', 'KNN', 'LR', 'RF', 'XGB', 'LGBM'], index = range(4))# Metricsresults.iloc[0, 2] = round(precision_score(y_test, y_pred), 2)results.iloc[1, 2] = round(recall_score(y_test, y_pred), 2)results.iloc[2, 2] = round(f1_score(y_test, y_pred), 2)results.iloc[3, 2] = round(roc_auc_score(y_test, y_prob), 3)lg_cm = confusion_matrix(y_test, y_pred)print(classification_report(y_test, y_pred))print(f'ROC AUC score: {round(roc_auc_score(y_test, y_prob), 3)}')print('')print('-----------------------------------------------------')print('')print('Cross-validation scores with 5 folds:')print('')print(f"ROC AUC: {round(cross_val_score(lg, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'roc_auc').mean(), 3)}")print(f"precision: {round(cross_val_score(lg, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'precision').mean(), 2)}")print(f"recall: {round(cross_val_score(lg, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'recall').mean(), 2)}")print(f"f1: {round(cross_val_score(lg, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'f1').mean(), 2)}")# Visualize confusion matrixplt.figure(figsize = (8, 5))sns.heatmap(lg_cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15},             yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])plt.yticks(rotation = 0)plt.show()# Roc curvefalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)roc_auc = auc(false_positive_rate, true_positive_rate)sns.set_theme(style = 'white')plt.figure(figsize = (8, 8))plt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)plt.legend(loc = 'lower right')plt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')plt.axis('tight')plt.ylabel('True Positive Rate')plt.xlabel('False Positive Rate')plt.show()#KNNknn = KNeighborsClassifier()knn.fit(X_train_balanced, Y_train_balanced)y_pred = knn.predict(X_test)y_prob = knn.predict_proba(X_test)[:,1]# Metricsresults.iloc[0, 1] = round(precision_score(y_test, y_pred), 2)results.iloc[1, 1] = round(recall_score(y_test, y_pred), 2)results.iloc[2, 1] = round(f1_score(y_test, y_pred), 2)results.iloc[3, 1] = round(roc_auc_score(y_test, y_prob), 3)knn_cm = confusion_matrix(y_test, y_pred)print(classification_report(y_test, y_pred))print(f'ROC AUC score: {round(roc_auc_score(y_test, y_prob), 3)}')print('')print('-----------------------------------------------------')print('')print('Cross-validation scores with 5 folds:')print('')print(f"ROC AUC: {round(cross_val_score(knn, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'roc_auc').mean(), 3)}")print(f"precision: {round(cross_val_score(knn, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'precision').mean(), 2)}")print(f"recall: {round(cross_val_score(knn, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'recall').mean(), 2)}")print(f"f1: {round(cross_val_score(knn, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'f1').mean(), 2)}")# Visualize confusion matrixplt.figure(figsize = (8, 5))sns.heatmap(knn_cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15},             yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])plt.yticks(rotation = 0)plt.show()# Roc curvefalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)roc_auc = auc(false_positive_rate, true_positive_rate)sns.set_theme(style = 'white')plt.figure(figsize = (8, 8))plt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)plt.legend(loc = 'lower right')plt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')plt.axis('tight')plt.ylabel('True Positive Rate')plt.xlabel('False Positive Rate')plt.show()#Random Forest Classifierrf = RandomForestClassifier(random_state = 22, max_depth = 5)rf.fit(X_train_balanced, Y_train_balanced)y_pred = rf.predict(X_test)y_prob = rf.predict_proba(X_test)[:,1]# Metricsresults.iloc[0, 3] = round(precision_score(y_test, y_pred), 2)results.iloc[1, 3] = round(recall_score(y_test, y_pred), 2)results.iloc[2, 3] = round(f1_score(y_test, y_pred), 2)results.iloc[3, 3] = round(roc_auc_score(y_test, y_prob), 3)rf_cm = confusion_matrix(y_test, y_pred)print(classification_report(y_test, y_pred))print(f'ROC AUC score: {round(roc_auc_score(y_test, y_prob), 3)}')print('')print('-----------------------------------------------------')print('')print('Cross-validation scores with 5 folds:')print('')print(f"ROC AUC: {round(cross_val_score(rf, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'roc_auc').mean(), 3)}")print(f"precision: {round(cross_val_score(rf, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'precision').mean(), 2)}")print(f"recall: {round(cross_val_score(rf, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'recall').mean(), 2)}")print(f"f1: {round(cross_val_score(rf, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'f1').mean(), 2)}")# Visualize confusion matrixplt.figure(figsize = (8, 5))sns.heatmap(rf_cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15},           yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])plt.yticks(rotation = 0)plt.show()# Roc curvefalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)roc_auc = auc(false_positive_rate, true_positive_rate)sns.set_theme(style = 'white')plt.figure(figsize = (8, 8))plt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)plt.legend(loc = 'lower right')plt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')plt.axis('tight')plt.ylabel('True Positive Rate')plt.xlabel('False Positive Rate')plt.show()#SVCsvc = SVC(random_state = 22, probability = True)svc.fit(X_train_balanced, Y_train_balanced)y_pred = svc.predict(X_test)y_prob = svc.predict_proba(X_test)[:,1]# Metricsresults.iloc[0, 0] = round(precision_score(y_test, y_pred), 2)results.iloc[1, 0] = round(recall_score(y_test, y_pred), 2)results.iloc[2, 0] = round(f1_score(y_test, y_pred), 2)results.iloc[3, 0] = round(roc_auc_score(y_test, y_prob), 3)svc_cm = confusion_matrix(y_test, y_pred)print(classification_report(y_test, y_pred))print(f'ROC AUC score: {round(roc_auc_score(y_test, y_prob), 3)}')print('')print('-----------------------------------------------------')print('')print('Cross-validation scores with 5 folds:')print('')print(f"ROC AUC: {round(cross_val_score(svc, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'roc_auc').mean(), 3)}")print(f"precision: {round(cross_val_score(svc, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'precision').mean(), 2)}")print(f"recall: {round(cross_val_score(svc, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'recall').mean(), 2)}")print(f"f1: {round(cross_val_score(svc, X_train_balanced, Y_train_balanced, cv = 5, scoring = 'f1').mean(), 2)}")# Visualize confusion matrixplt.figure(figsize = (8, 5))sns.heatmap(svc_cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15},             yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])plt.yticks(rotation = 0)plt.show()# Roc curvefalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)roc_auc = auc(false_positive_rate, true_positive_rate)sns.set_theme(style = 'white')plt.figure(figsize = (8, 8))plt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)plt.legend(loc = 'lower right')plt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')plt.axis('tight')plt.ylabel('True Positive Rate')plt.xlabel('False Positive Rate')plt.show() 